# -*- coding: utf-8 -*-
"""util.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tYmmhXqH5U-rLKw-lw_fTdG27Qxb_y8H
"""
import umap.umap_ as umap
from sklearn.manifold import TSNE
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import random
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Activation, Dropout, Flatten, Input, Dense, concatenate
from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, mean_squared_error
from tensorflow.keras.models import Model, Sequential, load_model
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import random
import os
import sys
import time
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from numpy import random
from scipy.fft import fft, ifft, fftfreq
import copy
import scipy
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import random
import os
import sys
import time
#import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from numpy import random
from scipy.fft import fft, ifft, fftfreq
import copy
import scipy
from sklearn.model_selection import train_test_split
import yaml
#import optuna
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
import random
from sklearn.ensemble import RandomForestRegressor
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Activation, Dropout, Flatten, Input, Dense, concatenate
from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, mean_squared_error, mean_absolute_percentage_error
from tensorflow.keras.models import Model, Sequential, load_model, model_from_json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
#import preprocessing
#import dataloader
#from preprocessing import preprocessor
#from dataloader import dataLoader
import sys
import os
import os.path
import csv
#import ensemRegressor
#import optunatransformator1
import math
import keras
import torch
import pickle
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
#import seaborn as sns
import latex
import numpy as np
import hdbscan
import matplotlib.pyplot as plt

def plot_losses(train_losses, val_losses, title, saving_path):
    epochs = range(1, len(train_losses) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_losses, 'bo', label='Training loss')
    plt.plot(epochs, val_losses, 'b', label='Validation loss')
    plt.title(title)
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(saving_path)

def save_variables(filename, **kwargs):
    with open(filename, 'wb') as f:
        pickle.dump(kwargs, f)
    print(f"Variables saved to {filename}")

def find_nearest_centroids(elements: torch.Tensor, centroids: torch.Tensor) -> torch.Tensor:
    """
    Finds the index of the nearest centroid for each element.
    
    Args:
        elements (torch.Tensor): Tensor of shape (n, 9), where n is the number of elements.
        centroids (torch.Tensor): Tensor of shape (m, 9), where m is the number of centroids.
    
    Returns:
        torch.Tensor: Tensor of shape (n,) containing the index of the closest centroid for each element.
    """
    # Compute the squared Euclidean distance between each element and each centroid
    distances = torch.cdist(elements, centroids, p=2)  # Shape: (n, m)
    
    # Get the index of the minimum distance (closest centroid) for each element
    nearest_centroid_indices = torch.argmin(distances, dim=1)    
    return nearest_centroid_indices


def split_dataframe(df, index_range, sample_size=10000, csv_filename='selected_data.csv', txt_filename='selected_indices.txt'):
    """
    Splits a DataFrame into two: one with randomly selected indices and another with remaining rows.
    Saves the selected DataFrame to a CSV file and selected indices to a TXT file.
    
    Args:
        df (pd.DataFrame): The input DataFrame.
        index_range (tuple): The range (min, max) to sample indices from.
        sample_size (int): The number of random indices to select.
        csv_filename (str): Filename to save the selected DataFrame.
        txt_filename (str): Filename to save the selected indices.
    
    Returns:
        pd.DataFrame, pd.DataFrame: Two DataFrames (selected subset, remaining subset)
    """
    # Generate 10,000 random indices within the given range
    selected_indices = np.random.choice(range(index_range[0], index_range[1]), sample_size, replace=False)
    
    # Ensure the indices are within the DataFrame index range
    selected_indices = np.intersect1d(selected_indices, df.index)
    
    # Extract selected and remaining rows
    selected_df = df.loc[selected_indices]
    remaining_df = df.drop(selected_indices)
    
    # Save selected DataFrame to CSV
    selected_df.to_csv(csv_filename, index=False)
    
    # Save selected indices to TXT file
    np.savetxt(txt_filename, selected_indices, fmt='%d')
    
    return selected_df, remaining_df


def load_variables(filename):
    with open(filename, 'rb') as f:
        data = pickle.load(f)
    print(f"Variables loaded from {filename}")
    return data

def save_model(model, model_name):
    torch.save(model.state_dict(), model_name)
    print("Model saved successfully.")
def load_model(model_class, model_name):
    model = model_class()
    model.load_state_dict(torch.load(model_name))
    model.eval()
    return model


def get_input_dim_from_encoder(encoder):
    # Inspect the first layer of the encoder to get the input dimension
    input_layer = encoder[0]
    if isinstance(input_layer, nn.Linear):
        return input_layer.in_features
    else:
        raise ValueError("The first layer of the encoder is not a Linear layer.")

def append_to_pickle(filename, **new_data):
    # Load the existing data
    data = load_variables(filename)

    # Add new variables to the data dictionary
    data.update(new_data)

    # Save the updated data back to the pickle file
    save_variables(filename, **data)

def visualize_clusters_2d(data, labels, outfile, title='Cluster Visualization', xlabel='Feature 1', ylabel='Feature 2'):
    """
    Visualize clusters in 2D.

    :param data: np.ndarray, the data points to visualize
    :param labels: np.ndarray, the cluster labels for each data point
    :param title: str, the title of the plot
    :param xlabel: str, the label for the x-axis
    :param ylabel: str, the label for the y-axis
    """
    unique_labels = np.unique(labels)
    palette = sns.color_palette("hsv", len(unique_labels))

    fig = go.Figure()

    for label, color in zip(unique_labels, palette):
        cluster_data = data[labels == label]
        fig.add_trace(go.Scatter(x=cluster_data[:, 0], y=cluster_data[:, 1], mode='markers',
                                 marker=dict(color=color), name=f'Cluster {label}'))

    fig.update_layout(title=title, xaxis_title=xlabel, yaxis_title=ylabel)
    fig.write_image(outfile, scale=2,
                  width=1280,
                  height=740)



def n_visualize_tsne(data, labels, outfile):
    #from sklearn.manifold import TSNE
    tsne = TSNE(n_components=2, random_state=42)
    tsne_data = tsne.fit_transform(data)
    scatter = plt.scatter(tsne_data[:, 0], tsne_data[:, 1], c=labels, cmap='viridis')
    plt.legend(*scatter.legend_elements())
    plt.title("Clusters Visualized using t-SNE")
    #plt.title(title)
    # plt.write(outfile)
    plt.savefig(outfile)


def n_visualize_umap(data, labels, outfile):
    reducer = umap.UMAP(n_components=2, random_state=42)
    umap_data = reducer.fit_transform(data)
    plt.scatter(umap_data[:, 0], umap_data[:, 1], c=labels, cmap='viridis')
    plt.title("Clusters Visualized using UMAP")
    plt.savefig(outfile)

def visualize_latent_space(encoded_data, cluster_labels, outfile, title='t-SNE Plot of Latent Space', perplexity=30, learning_rate=200, n_iter=1000):
    """
    Visualize the latent space using t-SNE with clusters represented by different colors.

    :param encoded_data: np.ndarray, the encoded data
    :param cluster_labels: np.ndarray, the cluster labels for the encoded data
    :param title: str, the title of the plot
    :param perplexity: int, t-SNE perplexity parameter
    :param learning_rate: int, t-SNE learning rate parameter
    :param n_iter: int, number of iterations for t-SNE
    """
    tsne = TSNE(perplexity=perplexity, learning_rate=learning_rate, n_iter=n_iter, random_state=0)
    tsne_results = tsne.fit_transform(encoded_data)

    # Create a DataFrame for easier plotting
    df_tsne = pd.DataFrame(tsne_results, columns=['TSNE1', 'TSNE2'])
    df_tsne['Cluster'] = cluster_labels

    # Plot using seaborn
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x='TSNE1', y='TSNE2', hue='Cluster', palette=sns.color_palette('hsv', len(np.unique(cluster_labels))), data=df_tsne, legend='full')
    plt.title(title)
    # plt.write(outfile)
    plt.savefig(outfile)
    df_tsne.to_csv("/work2/08389/hcs77/stampede3/application-fingerprinting/fig/actual_tsne.csv",index=False)

def dbscan_cluster_data(data):
    # Compute the clustering using HDBSCAN
    clusterer = hdbscan.HDBSCAN(min_cluster_size=3, min_samples=10000)
    clusterer.fit(data)
    cluster_labels = clusterer.labels_
    unique_labels = np.unique(cluster_labels)
    """
    cluster_centers = {}
    for label in unique_labels:
        cluster_data = data[cluster_labels == label]
        if len(cluster_data) > 0:  # Check if cluster has data points
            cluster_centers[label] = np.mean(cluster_data, axis=0) 
    """
    return cluster_labels, unique_labels 

    # Plot the results
    """
    plt.scatter(data[:, 0], data[:, 1], c=clusterer.labels_, cmap='rainbow')
    plt.title("HDBSCAN Clustering")
    plt.show()
    """

def cluster_data(encoded_data, num_clusters):
    """
    Cluster the encoded data using KMeans and return the cluster labels.

    :param encoded_data: np.ndarray, the encoded data
    :param num_clusters: int, the number of clusters
    :return: np.ndarray, the cluster labels
    """
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    cluster_labels = kmeans.fit_predict(encoded_data)
    unique_labels = np.unique(cluster_labels)
    centroids = kmeans.cluster_centers_
    return cluster_labels, unique_labels, centroids

def visualize_clusters(cluster_labels, outfile, title):
    """
    Visualize the number of jobs per cluster using a histogram.

    :param cluster_labels: np.ndarray, the cluster labels
    """
    fig = px.histogram(cluster_labels, nbins=len(np.unique(cluster_labels)), labels={'value': 'Cluster', 'count': 'Number of Jobs'})
    fig.update_layout(title=title, xaxis_title='Cluster', yaxis_title='Number of Jobs')
    #fig.show()
    fig.write_image(outfile, scale=2,
                  width=1280,
                  height=740)
def visualize_clusters_scatter(cluster_labels, outfile, title):
    """
    Visualize the number of jobs per cluster using a histogram.

    :param cluster_labels: np.ndarray, the cluster labels
    """
    fig = px.histogram(cluster_labels, nbins=len(np.unique(cluster_labels)), labels={'value': 'Cluster', 'count': 'Number of Jobs'})
    fig.update_layout(title=title, xaxis_title='Cluster', yaxis_title='Number of Jobs')
    #fig.show()
    fig.write_image(outfile, scale=2,
                  width=1280,
                  height=740)


def visualize_box_plot(dfSO, outfile, num_of_rows=0):
    if num_of_rows!= 0:
        dfSO = dfSO[0:num_of_rows]
    Epcs_array = []
    Difference_array = []
    for i in range(len(dfSO)):
        lst = dfSO["Difference"][i]
        print(f"lst[0] {lst[0]}")
        #lst = lst.replace("[",'')
        #lst = lst.replace("]",'')
        #lst = lst.split(" ")
        for j in range(len(lst)):
            Epcs_array.append(dfSO["Epcs"][i])
            if lst[j] == '':
                continue
            Difference_array.append(float(lst[j]))
    n_dfSO = pd.DataFrame(list(zip(Epcs_array, Difference_array)),columns =['Epcs', 'Difference'])
    print("n_dfSO")
    print(n_dfSO)
    fig = go.Figure()
    fig.add_trace(go.Box(x=n_dfSO['Epcs'],y=n_dfSO['Difference'], marker_color='palevioletred', fillcolor='pink',
                                name='Differences',boxpoints=False
                                #box_visible=True,
                               # legendgroup='Method 1', scalegroup='Method 1', name='Method 1',
                               # line_color='palevioletred',
                               #meanline_visible=True,
                                #opacity=0.7
                     ))
    print("plotting done")
    fig.update_layout(
      paper_bgcolor='rgba(1,1,1,0)',
      plot_bgcolor='rgba(1,1,1,0)',
      xaxis = dict(
        title="Samples",
        tickangle=0
    ),
     yaxis = dict(
        title="Box plot of errors per sample",
        #range = [yRangeS, yRangeF],
        tickmode = 'array'
    ),
      font=dict(
          family="Times New Roman",
          size=24,
          color="black"
      ),
    legend=dict(
    orientation="h",
    yanchor="top",
    y=1.2,
    xanchor="center",
    x=0.4
    )
    )
    fig.update_layout(boxmode='group')
    fig.write_image(outfile,scale=2,
                  width=1280,
                  height=740)
    fig.show()




def visualize_bar_plot(dfSO, outfile):
    Epcs_array = []
    Difference_array = []
    for i in range(len(dfSO)):
        lst = np.abs(dfSO["Difference"][i])
        print(f"lst[0] {lst[0]}")
        diff_array = []
        #lst = lst.replace("[",'')
        #lst = lst.replace("]",'')
        #lst = lst.split(" ")
        for j in range(len(lst)):
            #Epcs_array.append(dfSO["Epcs"][i])
            if lst[j] == '':
                continue
            diff_array.append(float(lst[j]))
        Epcs_array.append(dfSO["Epcs"][i])
        Difference_array.append(np.average(diff_array))
    df = pd.DataFrame(list(zip(Epcs_array, Difference_array)),columns =['Epcs', 'Difference'])

    fig = go.Figure()
    fig.add_trace(go.Bar(x=df['Epcs'],y=df['Difference'], marker_color='palevioletred',
                                name='difference',
                                 marker=dict(
                                  color='palevioletred',
                                  pattern = dict(shape = '+'),
                                  )
                                #box_visible=True,
                               # legendgroup='Method 1', scalegroup='Method 1', name='Method 1',
                               # line_color='palevioletred',
                               #meanline_visible=True,
                                #opacity=0.7
                     ))
    fig.update_layout(
      paper_bgcolor='rgba(1,1,1,0)',
      plot_bgcolor='rgba(1,1,1,0)',
      xaxis = dict(
        title="Samples",
        tickangle=0,
    ),
     yaxis = dict(
        title="Difference between actual and predictions",
        tickmode = 'array'
    ),
      font=dict(
          family="Times New Roman",
          size=24,
          color="black"
      ),
    legend=dict(
    orientation="h",
    yanchor="top",
    y=1.2,
    xanchor="center",
    x=0.6
    )
    )
    fig.update_layout(boxmode='group')
    fig.write_image(outfile,scale=2,
                  width=640,
                  height=370)
    fig.show()



def visualize_bar_plot2(xvals, yvals, outfile):

    fig = go.Figure()
    fig.add_trace(go.Bar(x=xvals, y=yvals, marker_color='palevioletred',
                                name='errors',
                                 marker=dict(
                                  color='palevioletred',
                                  pattern = dict(shape = '+'),
                                  )
                                #box_visible=True,
                               # legendgroup='Method 1', scalegroup='Method 1', name='Method 1',
                               # line_color='palevioletred',
                               #meanline_visible=True,
                                #opacity=0.7
                     ))
    fig.update_layout(
      paper_bgcolor='rgba(1,1,1,0)',
      plot_bgcolor='rgba(1,1,1,0)',
      xaxis = dict(
        title="+-50 from actual values",
        tickangle=0,
    ),
     yaxis = dict(
        title="count of values",
        tickmode = 'array'
    ),
      font=dict(
          family="Times New Roman",
          size=24,
          color="black"
      ),
    legend=dict(
    orientation="h",
    yanchor="top",
    y=1.2,
    xanchor="center",
    x=0.6
    )
    )
    fig.update_layout(boxmode='group')
    fig.write_image(outfile,scale=2,
                  width=640,
                  height=370)
    fig.show()




"""
class deltaUQ(keras.Model):
  def call(self, inputs):
      x = self.block_1(inputs)
      x = self.block_2(x)
      x = self.global_pool(x)
      return self.classifier(x)

  def set_parms(self, anchors=None, n_anchors=1, return_std=False):
    #super(deltaUQ, self).__init__()
    self.anchors = anchors
    # best_weights to store the weights at which the minimum loss occurs.
    self.n_anchors = n_anchors
    self.return_std = return_std
  def create_anchored_batch(self,x,anchors=None,n_anchors=1,corrupt=False):
    '''
    anchors (default=None):
    if passed, will use the same set of anchors for all batches during training.
    if  None, we will use a shuffled input minibatch to forward( ) as anchors for that batch (random)

    During *inference* it is recommended to keep the anchors fixed for all samples.

    n_anchors is chosen as min(n_batch,n_anchors)
    '''
    print("x")
    print(x)
    n_samples = x.shape[0]
    if anchors is None:
      #anchors = x[np.random.randint(n_samples, size=(n_samples,)),:]
      anchors =  tf.random.shuffle(x, seed=None, name=None)
      anchors = anchors.numpy()

    ## make anchors (n_anchors) --> n_img*n_anchors
    print("Num of samples")
    print(n_samples)
    print("*****************Anchors***************")
    print(anchors)
    print(anchors.shape)
    if self.training:
      A = anchors[np.random.randint(anchors.shape[0],(n_samples*n_anchors,)),:]
    else:
      anchors =  tf.random.shuffle(anchors, seed=None, name=None)
      anchors = anchors.numpy()
      A = np.repeat(anchors[0:n_anchors,:], n_samples, axis=0)

    if corrupt:
      refs = self.corruption(A)
    else:
      refs = A

    ## before computing residual, make minibatch (n_img) --> n_img* n_anchors
    if len(x.shape)<=2:
      #diff = x.tile((n_anchors,1))
      diff = np.tile(x, (n_anchors,1))
      assert diff.shape[1]==A.shape[1], f"Tensor sizes for `diff`({diff.shape}) and `anchors` ({A.shape}) don't match!"
      diff -= A
    else:
      diff = np.tile(x, (n_anchors,1,1,1)) - A
      #diff = x.tile((n_anchors,1,1,1)) - A

    #batch = torch.cat([refs,diff],axis=1)
    batch = np.concatenate((refs, diff), axis=1)
    return batch

  def train_step(self, data):
    # Unpack the data. Its structure depends on your model and
    # on what you pass to `fit()`.
    x, y = data
    a_batch = self.create_anchored_batch(x, anchors=self.anchors,n_anchors=self.n_anchors)
    with tf.GradientTape() as tape:
      y_pred = self(x, training=True)  # Forward pass
      # Compute the loss value
      # (the loss function is configured in `compile()`)
      y_pred = y_pred.reshape(self.n_anchors, x.shape[0], y_pred.shape[1])
      y_pred = y_pred.mean(0)
      loss = self.compute_loss(y=y, y_pred=y_pred)

    # Compute gradients
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)
    # Update weights
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    # Update metrics (includes the metric that tracks the loss)
    for metric in self.metrics:
      if metric.name == "loss":
        metric.update_state(loss)
      else:
        metric.update_state(y, y_pred)
    # Return a dict mapping metric names to current value
    return {m.name: m.result() for m in self.metrics}
"""
def create_deltaUQ(
                 neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment = 0.5, actF="relu", lossF="mean_squared_error", transfer=False, frozen_layers = 0, num_of_outputs = 1):
  inputs = keras.Input(103,)
  for i in range(num_of_layers_1):
    inputs = Dense(units=neurons_input, activation=actF)(inputs)
    inputs = BatchNormalization(momentum=moment)(inputs)
    #model.add(Dropout(0.4)
  outputs = Dense(units=num_of_outputs)(inputs)
  model = deltaUQ(inputs, outputs)
  #, kernel_regularizer=tf.keras.regularizers.l1(0.01),
  #                            activity_regularizer=tf.keras.regularizers.l2(0.01)))
  opt1 = tf.keras.optimizers.Nadam(learning_rate=lr)
  model.compile(loss=lossF, optimizer=opt1, metrics=['mse','mae','mape'])
  return model

def create_model4(
                 neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment = 0.5, actF="relu", lossF="mean_squared_error", transfer=False, frozen_layers = 0, num_of_outputs = 1):

  model = Sequential()
  for i in range(num_of_layers_1):
    layer = Dense(units=neurons_input, activation=actF)
    layer2 = BatchNormalization(momentum=moment)
    if transfer == True and i< frozen_layers:
      #(num_of_layers_1-1):
      layer.trainable=False
      layer2.trainable=False
    model.add(layer)
    model.add(layer2)
    #model.add(Dropout(0.4))
  finalLayer = Dense(units=num_of_outputs)
  #finalLayer.trainable=False
  model.add(finalLayer)
  #, kernel_regularizer=tf.keras.regularizers.l1(0.01),
  #                            activity_regularizer=tf.keras.regularizers.l2(0.01)))
  opt1 = tf.keras.optimizers.Nadam(learning_rate=lr)
  model.compile(loss=lossF, optimizer=opt1, metrics=['mse','mae','mape'])
  return model

class ResNet(keras.Model):

    def __init__(self, num_classes=1000):
        super().__init__()
        self.block_1 = ResNetBlock()
        self.block_2 = ResNetBlock()
        self.global_pool = layers.GlobalAveragePooling2D()
        self.classifier = Dense(num_classes)

    def call(self, inputs):
        x = self.block_1(inputs)
        x = self.block_2(x)
        x = self.global_pool(x)
        return self.classifier(x)

class CustomLayer(keras.layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.out = 0

    def call(self, inputs, labels, training=False):
        if training:
            if mean_squared_error(labels,inputs[0])<mean_squared_error(labels,inputs[1]):
                return inputs[0]
            else:
                self.out = 1
                return inputs[1]
        else:
            if self.out == 0:
                return inputs[0]
            else:
                return inputs[1]



class SumNet(Model):

    def __init__(self, labels):
        super().__init__()
        self.block_1 = CustomLayer()
        self.labels = labels

    def call(self, inputs, training=False):
        return self.block_1(inputs, self.labels, training)



def create_additioner(
                 neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment = 0.5, actF="relu", lossF="mean_squared_error", num_of_outputs = 1):

  model = Sequential()
  """
  for i in range(num_of_layers_1):
    layer = Dense(units=neurons_input, activation=actF)
    layer2 = BatchNormalization(momentum=moment)
    if transfer == True and i< frozen_layers:
      #(num_of_layers_1-1):
      layer.trainable=False
      layer2.trainable=False
    model.add(layer)
    model.add(layer2)
    #model.add(Dropout(0.4))
  """
  #layer = tf.keras.layers.Add() #([x, x_skip])
  #model.add(layer)
  #input1 = keras.layers.Input(shape=(2, ))
  #model.add(input1)
  finalLayer = Dense(units = num_of_outputs,
          #input_shape=(2,), 
          activation=actF)
  #finalLayer.trainable=False
  model.add(finalLayer)
  #, kernel_regularizer=tf.keras.regularizers.l1(0.01),
  #                            activity_regularizer=tf.keras.regularizers.l2(0.01)))
  opt1 = tf.keras.optimizers.Nadam(learning_rate=lr)
  model.compile(loss=lossF, optimizer=opt1, metrics=['mse','mae','mape'])
  return model


def newscatterPlot(ground_truth, predictions, saving_path, ground_truth_filename, predictions_filename, title):
  os.makedirs(os.path.dirname(saving_path), exist_ok=True)
  os.makedirs(os.path.dirname(ground_truth_filename), exist_ok=True)
  fileI = open(ground_truth_filename, "w")
  writerI = csv.writer(fileI)
  for a in ground_truth:
    writerI.writerow(a)
  fileI.close()
  os.makedirs(os.path.dirname(predictions_filename), exist_ok=True)
  fileI = open(predictions_filename, "w")
  writerI = csv.writer(fileI)
  for a in predictions:
    writerI.writerow(a)
  fileI.close()
  matplotlib.rcParams['mathtext.fontset'] = 'stix'
  matplotlib.rcParams['font.family'] = 'STIXGeneral'
  plt.figure(figsize=(10,10))
  plt.scatter(ground_truth, predictions)
  mse,r2 = mean_squared_error(ground_truth#.reshape(-1,1)
                , predictions), r2_score(ground_truth#.reshape(-1,1)
                                , predictions)
  plt.xlim([0, 1])
  #plt.ylim([-5, 20]) 
  plt.xlabel('Actual Labels', fontsize=32)
  plt.ylabel('Predicted Labels', fontsize=32)
  plt.xticks(fontsize=32)
  plt.yticks(fontsize=32)
  plt.title(f'MSE: {mse}, R2: {r2}-{title}')
  plt.savefig(saving_path)


class EarlyStoppingAtMinLoss(tf.keras.callbacks.Callback):
  """Stop training when the loss is at its min, i.e. the loss stops decreasing.

  Arguments:
      patience: Number of epochs to wait after min has been hit. After this
      number of no improvement, training stops.
  """
  def __init__(self, patience=0, arg_loss="val_loss"):
    super(EarlyStoppingAtMinLoss, self).__init__()
    self.patience = patience
    # best_weights to store the weights at which the minimum loss occurs.
    self.best_weights = None
    self.targeted_loss = arg_loss

  def on_train_begin(self, logs=None):
    # The number of epoch it has waited when loss is no longer minimum.
    self.wait = 0
    # The epoch the training stops at.
    self.stopped_epoch = 0
    # Initialize the best as infinity.
    self.best = np.Inf

  def on_epoch_end(self, epoch, logs=None):
    current = logs.get(self.targeted_loss)
    if np.less(current, self.best):
      self.best = current
      self.wait = 0
      # Record the best weights if current results is better (less).
      self.best_weights = self.model.get_weights()
    else:
      self.wait += 1
      if self.wait >= self.patience:
        self.stopped_epoch = epoch
        self.model.stop_training = True
        print("Restoring model weights from the end of the best epoch.")
        self.model.set_weights(self.best_weights)

  def on_train_end(self, logs=None):
    if self.stopped_epoch > 0:
      print("Epoch %05d: early stopping" % (self.stopped_epoch + 1))

def sampleMaker(tar_x_scaled, tar_y_scaled, path, n):
  os.makedirs(os.path.dirname(path), exist_ok=True)
  indices = open(path, "w" )
  readModeOn = False
  dropIndices = []
  x2 =[]
  lb2 = []
  totallen = len(tar_x_scaled)
  if isinstance(n, float):
    num_of_samples = int(math.ceil(n*len(tar_x_scaled)))
  else:
    num_of_samples = n
  for k in range(num_of_samples):
    index = random.randint(0, totallen-1 ) #int(nums[i])
    while index in dropIndices:
      index = random.randint(0, totallen-1 )
    indices.write(str(index))
    indices.write(" ")
    indices.write("\n")
    #print(index)
    dropIndices.append(index)
    if len(tar_x_scaled[index:index+1])> 0:
      x2.append(tar_x_scaled[index:index+1])
      lb2.append(tar_y_scaled[index:index+1])
  #dropIndices = list([*set(dropIndices)])
  #print(f"after removing duplicates dropIndices {dropIndices}")
  drop_len = len(dropIndices)
  print(f"drop indices length {drop_len}")
  print(f"num of samples needed to be added {num_of_samples}")
  for k in range(drop_len):
    idx = dropIndices.pop(0)
    for d in range(len(dropIndices)):
      if dropIndices[d]> idx:
        dropIndices[d] -= 1
    #print(f" dropIndices {dropIndices}")
    #print(f" len of tar_x_scaled {len(tar_x_scaled)}")
    #print(f"idx {idx}")
    tar_x_scaled = np.delete(tar_x_scaled, idx, 0)
    tar_y_scaled = np.delete(tar_y_scaled, idx, 0)
  indices.close()
  """
  print("inside fine tuning x2 shape")
  print(f"x2 shape {x2.shape}")
  """
  print(x2)
  """
  print(f"x2 shape {x2.shape}")
  """
  x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
  lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
  x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
  lb2 = tf.reshape(lb2, (lb2.shape[0], lb2.shape[2]))
  return x2, lb2, tar_x_scaled, tar_y_scaled


def n_sampleMaker(tar_x_scaled, tar_y_scaled, path, n):
  os.makedirs(os.path.dirname(path), exist_ok=True)
  indices = open(path, "w" )
  readModeOn = False
  dropIndices = []
  x2 =[]
  lb2 = []
  totallen = len(tar_x_scaled)
  if isinstance(n, float):
    num_of_samples = int(math.ceil(n*len(tar_x_scaled)))
  else:
    num_of_samples = n
  for k in range(num_of_samples):
    index = random.randint(0, totallen-1 ) #int(nums[i])
    while index in dropIndices:
      index = random.randint(0, totallen-1 )
    indices.write(str(index))
    indices.write(" ")
    indices.write("\n")
    #print(index)
    dropIndices.append(index)
    if len(tar_x_scaled[index:index+1])> 0:
      x2.append(tar_x_scaled[index:index+1])
      lb2.append(tar_y_scaled[index:index+1])
  #dropIndices = list([*set(dropIndices)])
  #print(f"after removing duplicates dropIndices {dropIndices}")
  drop_len = len(dropIndices)
  print(f"drop indices length {drop_len}")
  print(f"num of samples needed to be added {num_of_samples}")
  for k in range(drop_len):
    idx = dropIndices.pop(0)
    for d in range(len(dropIndices)):
      if dropIndices[d]> idx:
        dropIndices[d] -= 1
    #print(f" dropIndices {dropIndices}")
    #print(f" len of tar_x_scaled {len(tar_x_scaled)}")
    #print(f"idx {idx}")
    tar_x_scaled = np.delete(tar_x_scaled, idx, 0)
    tar_y_scaled = np.delete(tar_y_scaled, idx, 0)
  indices.close()
  print("inside fine tuning x2 shape")
  print(f"x2 shape {x2.shape}")
  print(x2)
  print(f"x2 shape {x2.shape}")
  x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
  lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
  x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
  lb2 = tf.reshape(lb2, (lb2.shape[0], lb2.shape[2]))
  return x2, lb2, tar_x_scaled, tar_y_scaled



def sampleLoader(tar_x_scaled, tar_y_scaled, path, n):
  dropIndices = []
  x2 =[]
  lb2 = []
  if os.path.isfile(path):
    indices = open(path, "r" )
    totallen = len(tar_x_scaled)
    print(f"n is {n}")
    if isinstance(n, float):
      num_of_samples = int(math.ceil(n*len(tar_x_scaled)))  #int(math.ceil(n*len(tar_x_scaled)))
    else:
      num_of_samples = n
    print(f"len of tar_x_scaled {len(tar_x_scaled)}")
    print(f"num of samples {num_of_samples}")
    loaded_indices = indices.readlines()
    print(f"loaded indices {loaded_indices}")
    for k in range(num_of_samples):
      index =  int(loaded_indices[k]) #
      print(f"index is {index}")
      dropIndices.append(index)
      if len(tar_x_scaled[index:index+1])> 0:
        x2.append(tar_x_scaled[index:index+1])
        lb2.append(tar_y_scaled[index:index+1])
    dropIndices = list([*set(dropIndices)])
    drop_len = len(dropIndices)
    print(f"dropindices is {dropIndices}")
    print(f" drop len is {drop_len}")
    #tar_x_scaled = tar_x_scaled.drop(dropIndices)
    for k in range(drop_len):
      idx = dropIndices.pop(0)
      for d in range(len(dropIndices)):
        if dropIndices[d]> idx:
          dropIndices[d] -= 1
      tar_x_scaled = np.delete(tar_x_scaled, idx, 0)
      tar_y_scaled = np.delete(tar_y_scaled, idx, 0)
    indices.close()
    #tar_x_scaled = tar_x_scaled.drop(dropIndices)
    # print(f"x2 {x2}")
    #print(f"x2 shape {x2.shape}")
    x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
    lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
    x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
    lb2 = tf.reshape(lb2, (lb2.shape[0], lb2.shape[2]))
  return x2, lb2, tar_x_scaled, tar_y_scaled



def n_sampleLoader(tar_x_scaled, tar_y_scaled, path, n):
  dropIndices = []
  x2 =[]
  lb2 = []
  if os.path.isfile(path):
    indices = open(path, "r" )
    totallen = len(tar_x_scaled)
    print(f"n is {n}")
    if isinstance(n, float):
      num_of_samples = int(math.ceil(n*len(tar_x_scaled)))
    else:
      num_of_samples = n
    print(f"len of tar_x_scaled {len(tar_x_scaled)}")
    print(f"num of samples {num_of_samples}")
    loaded_indices = indices.readlines()
    print(f"loaded indices {loaded_indices}")
    for k in range(num_of_samples):
      index =  int(loaded_indices[k]) #
      print(f"index is {index}")
      dropIndices.append(index)
      if len(tar_x_scaled[index:index+1])> 0:
        #x2.append(tar_x_scaled[index:index+1])
        lb2.append(tar_y_scaled[index:index+1])
    dropIndices = list([*set(dropIndices)])
    x2 = tar_x_scaled.iloc[dropIndices]
    drop_len = len(dropIndices)
    print(f"dropindices is {dropIndices}")
    print(f" drop len is {drop_len}")
    #tar_x_scaled = tar_x_scaled.drop(dropIndices)
    tar_x_scaled = tar_x_scaled.drop(tar_x_scaled.index[dropIndices])
    for k in range(drop_len):
      idx = dropIndices.pop(0)
      for d in range(len(dropIndices)):
        if dropIndices[d]> idx:
          dropIndices[d] -= 1
      #tar_x_scaled = np.delete(tar_x_scaled, idx, 0)
      tar_y_scaled = np.delete(tar_y_scaled, idx, 0)
    indices.close()
    print(f"x2 {x2}")
    print(f"x2 shape {x2.shape}")
    x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
    lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
    print(f"after converting to tensor x2 shape {x2.shape}")
    #x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
    lb2 = tf.reshape(lb2, (lb2.shape[0], lb2.shape[2]))
  return x2, lb2, tar_x_scaled, tar_y_scaled


def ript_sampleLoader(tar_x_scaled, tar_y_scaled, path, n):
  dropIndices = []
  x2 =[]
  lb2 = []
  if os.path.isfile(path):
    indices = open(path, "r" )
    totallen = len(tar_x_scaled)
    print(f"n is {n}")
    if isinstance(n, float):
      num_of_samples = int(math.ceil(n*len(tar_x_scaled)))
    else:
      num_of_samples = n
    print(f"len of tar_x_scaled {len(tar_x_scaled)}")
    print(f"num of samples {num_of_samples}")
    loaded_indices = indices.readlines()
    print(f"loaded indices {loaded_indices}")
    for k in range(num_of_samples):
      index =  int(loaded_indices[k]) #
      print(f"index is {index}")
      dropIndices.append(index)
      if len(tar_x_scaled[index:index+1])> 0:
        #x2.append(tar_x_scaled[index:index+1])
        lb2.append(tar_y_scaled[index:index+1])
    dropIndices = list([*set(dropIndices)])
    x2 = tar_x_scaled.iloc[dropIndices]
    drop_len = len(dropIndices)
    print(f"dropindices is {dropIndices}")
    print(f" drop len is {drop_len}")
    #tar_x_scaled = tar_x_scaled.drop(dropIndices)
    tar_x_scaled = tar_x_scaled.drop(tar_x_scaled.index[dropIndices])
    for k in range(drop_len):
      idx = dropIndices.pop(0)
      for d in range(len(dropIndices)):
        if dropIndices[d]> idx:
          dropIndices[d] -= 1
      #tar_x_scaled = np.delete(tar_x_scaled, idx, 0)
      tar_y_scaled = np.delete(tar_y_scaled, idx, 0)
    indices.close()
    print(f"x2 {x2}")
    print(f"x2 shape {x2.shape}")
    #x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
    lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
    print(f"after converting to tensor x2 shape {x2.shape}")
    #x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
    lb2 = tf.reshape(lb2, (lb2.shape[0], lb2.shape[2]))
  return x2, lb2, tar_x_scaled, tar_y_scaled



def tfSampleLoader(target_dataframe, path, n):
  dropIndices = []
  x2 =[]
  lb2 = []
  if os.path.isfile(path):
    indices = open(path, "r" )
    totallen = len(target_dataframe)
    if isinstance(n, float):
      num_of_samples = int(n*len(target_dataframe))
    else:
      num_of_samples = n
    loaded_indices = indices.readlines()
    for k in range(num_of_samples):
      index =  int(loaded_indices[k]) #
      dropIndices.append(index)
      if len(target_dataframe[index:index+1])> 0:
        #if k==0:
        x2.append(target_dataframe[index:index+1].values)
        #else:
        #x2 = pd.concat([x2, target_dataframe[index:index+1]], axis=1)
    print(f"x2 {x2}")
    dropIndices = list([*set(dropIndices)])
    drop_len = len(dropIndices)
    x2 = np.reshape(x2, (drop_len, 18))
    print(f"modified x2 {x2}")
    x2 = pd.DataFrame(x2, columns=target_dataframe.columns)
    print(f"dropindices is {dropIndices}")
    print(f" drop len is {drop_len}")
    fi = dropIndices[0]
    print(f"first sample is {target_dataframe[fi:fi+1]}")
    for k in range(drop_len):
      idx = dropIndices.pop(0)
      for d in range(len(dropIndices)):
        if dropIndices[d]> idx:
          dropIndices[d] -= 1
      target_dataframe.drop(target_dataframe.index[idx], axis=0, inplace=True)
      #target_dataframe = np.delete(target_dataframe, idx, 0)
    indices.close()

    tuning_samples = x2 #target_dataframe.iloc[dropIndices]
    training_samples = target_dataframe #.drop(dropIndices, axis=0)
  return tuning_samples, training_samples

def sourceModelLoader(source_features, source_labels, transfer_permission, num_of_frozen_layers, build_now, global_config, target_app, save_bit, output_n=1):
  f = open(os.getcwd()+global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  callback2 =  tf.keras.callbacks.EarlyStopping(monitor='loss', patience=40)
  batch_size = int(nums[3])
  if batch_size> len(source_features):
    batch_size = None
  #model.fit(source_features, source_labels, batch_size = int(nums[3]) , epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True)
  if build_now:
    loaded_model = create_model4(neurons_input = int(nums[0]) , num_of_layers_1= int(nums[1]), lr= float(nums[2]) , moment = float(nums[4]) , actF="relu", lossF="mean_squared_error", transfer=transfer_permission, frozen_layers= num_of_frozen_layers, num_of_outputs = output_n)
    if len(source_features)<10:
      print("Building model based on training loss")
      callback2 = EarlyStoppingAtMinLoss(patience = 40, arg_loss="loss") #
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2])
    else:
      print("Building model based on validation loss")
      callback2 = EarlyStoppingAtMinLoss(patience=40, arg_loss="val_loss")
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True )
    #loaded_model.fit(source_features, source_labels, batch_size = batch_size , epochs=1000, callbacks=[ callback2]) #, validation_split=0.2, shuffle= True)
    #model.save_weights(source_model_weights)
    if save_bit:
      os.makedirs(os.path.dirname(os.getcwd()+global_config['source_model']), exist_ok=True)
      model_json = loaded_model.to_json()
      with open(os.getcwd()+global_config['source_model'], "w") as json_file:
        json_file.write(model_json)
      # serialize weights to HDF5
      loaded_model.save_weights(os.getcwd()+global_config['source_model_weights'])
      print("Saved model to disk")
      json_file.close()
  else:
    #model.load_weights(source_model_weights)
    json_file = open(os.getcwd()+global_config['source_model'], 'r')
    loaded_model_json = json_file.read()

    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    # load weights into new model
    loaded_model.load_weights(os.getcwd()+global_config['source_model_weights'])
    print("Loaded model from disk")
    i = 0
    if transfer_permission == True:
      while i< (2*num_of_frozen_layers):
        loaded_model.layers[i].trainable=False
        loaded_model.layers[i].trainable=False
        i += 2
  opt1 = tf.keras.optimizers.Nadam(learning_rate=float(nums[2]))
  loaded_model.compile(loss='mean_squared_error', optimizer= opt1, metrics=['mse'])
  print("Model Building Done")
  return loaded_model
def deltaUQSourceLoader(source_features, source_labels, transfer_permission, num_of_frozen_layers, build_now, global_config, target_app, save_bit, anchors, n_anchors, output_n=1):
  f = open(os.getcwd()+global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  callback2 =  tf.keras.callbacks.EarlyStopping(monitor='loss', patience=40)
  batch_size = int(nums[3])
  if batch_size> len(source_features):
    batch_size = None
  #model.fit(source_features, source_labels, batch_size = int(nums[3]) , epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True)
  h = source_features
  make_anchors = lambda h: h.min()+torch.rand(1000,1)*(h.max()-h.min())
  anchors = make_anchors(h)
  if build_now:
    loaded_model = create_deltaUQ(neurons_input = int(nums[0]) , num_of_layers_1= int(nums[1]), lr= float(nums[2]) , moment = float(nums[4]) , actF="relu", lossF="mean_squared_error", transfer=transfer_permission, frozen_layers= num_of_frozen_layers, num_of_outputs = output_n)
    loaded_model.set_parms(anchors=anchors, n_anchors=n_anchors, return_std=False)
    loaded_model.training=True
    print("source features")
    print(source_features)
    if len(source_features)<10:
      print("Building model based on training loss")
      callback2 = EarlyStoppingAtMinLoss(patience = 40, arg_loss="loss") #
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2])
    else:
      print("Building model based on validation loss")
      callback2 = EarlyStoppingAtMinLoss(patience=40, arg_loss="val_loss")
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True )
    #loaded_model.fit(source_features, source_labels, batch_size = batch_size , epochs=1000, callbacks=[ callback2]) #, validation_split=0.2, shuffle= True)
    #model.save_weights(source_model_weights)
    if save_bit:
      os.makedirs((os.getcwd()+global_config['source_model']), exist_ok=True)
      model_json = loaded_model.to_json()
      with open(global_config['source_model'], "w") as json_file:
        json_file.write(model_json)
      # serialize weights to HDF5
      loaded_model.save_weights(os.getcwd()+global_config['source_model_weights'])
      print("Saved model to disk")
      json_file.close()
  else:
    #model.load_weights(source_model_weights)
    json_file = open(global_config['source_model'], 'r')
    loaded_model_json = json_file.read()

    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    # load weights into new model
    loaded_model.load_weights(global_config['source_model_weights'])
    print("Loaded model from disk")
    i = 0
    if transfer_permission == True:
      while i< (2*num_of_frozen_layers):
        loaded_model.layers[i].trainable=False
        loaded_model.layers[i].trainable=False
        i += 2
  opt1 = tf.keras.optimizers.Nadam(learning_rate=float(nums[2]))
  loaded_model.compile(loss='mean_squared_error', optimizer= opt1, metrics=['mse'])
  print("Model Building Done")
  return loaded_model
def targetModelLoader(source_features, source_labels, transfer_permission, num_of_frozen_layers, build_now, global_config, target_app, save_bit, callback_patience, train_epoch):
  
  f = open(os.getcwd()+global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  
  #nums = global_config['target_model_params']
  callback2 =  tf.keras.callbacks.EarlyStopping(monitor='loss', patience=40)
  batch_size = int(nums[3])
  if batch_size> len(source_features):
    batch_size = None
  #model.fit(source_features, source_labels, batch_size = int(nums[3]) , epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True)
  if build_now:
    loaded_model = create_model4(neurons_input = int(nums[0]) , num_of_layers_1= int(nums[1]), lr= float(nums[2]) , moment = float(nums[4]) , actF="relu", lossF="mean_squared_error", transfer=transfer_permission, frozen_layers= num_of_frozen_layers)
    #if len(source_features)<10:
    print("Building model based on training loss")
    callback2 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=callback_patience)# EarlyStoppingAtMinLoss(patience = callback_patience, arg_loss="loss") #
    loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs= train_epoch, callbacks=[ callback2])
    """
    else:
      print("Building model based on validation loss")
      callback2 = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=callback_patience) #EarlyStoppingAtMinLoss(patience= callback_patience, arg_loss="val_loss")
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs= train_epoch, callbacks=[ callback2], validation_split=0.2, shuffle= True )
    #loaded_model.fit(source_features, source_labels, batch_size = batch_size , epochs=1000, callbacks=[ callback2]) #, validation_split=0.2, shuffle= True)
    #model.save_weights(source_model_weights)
    
    if save_bit:
      os.makedirs(os.path.dirname(global_config['source_model']), exist_ok=True)
      model_json = loaded_model.to_json()
      with open(global_config['source_model'], "w") as json_file:
        json_file.write(model_json)
      # serialize weights to HDF5
      loaded_model.save_weights(global_config['source_model_weights'])
      print("Saved model to disk")
      json_file.close()
    """
  else:
    #model.load_weights(source_model_weights)
    json_file = open(global_config['source_model'], 'r')
    loaded_model_json = json_file.read()

    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    # load weights into new model
    loaded_model.load_weights(global_config['source_model_weights'])
    print("Loaded model from disk")
    i = 0
    if transfer_permission == True:
      while i< (2*num_of_frozen_layers):
        loaded_model.layers[i].trainable=False
        loaded_model.layers[i].trainable=False
        i += 2
  opt1 = tf.keras.optimizers.Nadam(learning_rate=float(nums[2]))
  loaded_model.compile(loss='mean_squared_error', optimizer= opt1, metrics=['mse'])
  print("Model Building Done")
  return loaded_model
def update_source_model(source_model, target_features, target_labels, fine_tune_sample_features, fine_tune_sample_labels, global_config, epoch_num):
  f = open(os.getcwd()+global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  batch_size = int(nums[3])
  predictions0 = source_model.predict(target_features)
  mse0 = mean_squared_error(target_labels, predictions0)
  if batch_size> len(fine_tune_sample_features):
    batch_size = None
  if len(fine_tune_sample_features)<10:
    """
    ls = len(fine_tune_sample_features)
    if ls<=2:
      epoch_num = 20
    elif ls>2 and ls<=7:
      epoch_num = 50
    elif ls >8 and ls<=10:
      epoch_num = 100
    """
    #epoch_num=20
    callback2 = EarlyStoppingAtMinLoss(patience = 40, arg_loss="loss") #
    source_model.fit(fine_tune_sample_features, fine_tune_sample_labels, batch_size = batch_size, epochs=epoch_num, callbacks=[ callback2])
  else:
    callback2 = EarlyStoppingAtMinLoss(patience=40, arg_loss="val_loss")
    source_model.fit(fine_tune_sample_features, fine_tune_sample_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True )
  predictions2 = source_model.predict(target_features)
  mse2 = mean_squared_error(target_labels, predictions2)
  return mse0, mse2

def getupdate_source_model(source_model, target_features, target_labels, fine_tune_sample_features, fine_tune_sample_labels, global_config):
  f = open(global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  batch_size = int(nums[3])
  predictions0 = source_model.predict(target_features)
  mse0 = mean_squared_error(target_labels, predictions0)
  if batch_size> len(fine_tune_sample_features):
    batch_size = None
  if len(fine_tune_sample_features)<10:
    callback2 = EarlyStoppingAtMinLoss(patience = 40, arg_loss="loss") #
    source_model.fit(fine_tune_sample_features, fine_tune_sample_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2])
  else:
    callback2 = EarlyStoppingAtMinLoss(patience=40, arg_loss="val_loss")
    source_model.fit(fine_tune_sample_features, fine_tune_sample_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True )
  predictions2 = source_model.predict(target_features)
  mse2 = mean_squared_error(target_labels, predictions2)
  return mse0, mse2, source_model


def iptupdate_source_model(source_model, fine_tune_sample_features, fine_tune_sample_labels, global_config, epoch_num):
  f = open(os.getcwd()+global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  batch_size = int(nums[3])
  #predictions0 = source_model.predict(fine_tune_sample_features )
  #mse0 = mean_squared_error(fine_tune_sample_labels, predictions0)
  mse0 = 0
  if batch_size> len(fine_tune_sample_features):
    batch_size = None
  #if len(fine_tune_sample_features)<10:
  callback2 = EarlyStoppingAtMinLoss(patience = 40, arg_loss="loss") #
  source_model.fit(np.nan_to_num(fine_tune_sample_features), fine_tune_sample_labels, batch_size = batch_size, epochs=epoch_num, callbacks=[ callback2])
  #else:
  #callback2 = EarlyStoppingAtMinLoss(patience=40, arg_loss="val_loss")
  #source_model.fit(fine_tune_sample_features, fine_tune_sample_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True )
  print("Model fitting done")
  predictions2 = source_model(fine_tune_sample_features)
  #mse2 = mean_squared_error(fine_tune_sample_labels, predictions2)
  mse2 = 0
  return predictions2, mse0, mse2, source_model






def yaml_key_adder(config, key, value):
  yml_config =f"{key}: {value}"
  with open(config, "a") as f:
    f.write(yml_config)
  f.close()
def scatterPlot(ground_truth, predictions, saving_path, title):
  os.makedirs(os.path.dirname(saving_path), exist_ok=True)
  matplotlib.rcParams['mathtext.fontset'] = 'stix'
  matplotlib.rcParams['font.family'] = 'STIXGeneral'
  plt.figure(figsize=(10,10))
  plt.scatter(ground_truth, predictions.ravel())
  mse,r2 = mean_squared_error(ground_truth#.reshape(-1,1)
                , predictions), r2_score(ground_truth#.reshape(-1,1)
                                , predictions)
  plt.xlabel('Actual Labels', fontsize=32)
  plt.ylabel('Predicted Labels', fontsize=32)
  plt.xticks(fontsize=32)
  plt.yticks(fontsize=32)
  plt.title(f'MSE: {mse}, R2: {r2}-{title}')
  plt.savefig(saving_path)

def plot_graphs(history, string, filename):
  os.makedirs(os.path.dirname(filename), exist_ok=True)
  matplotlib.rcParams['mathtext.fontset'] = 'stix'
  matplotlib.rcParams['font.family'] = 'STIXGeneral'
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xticks(fontsize=32)
  plt.yticks(fontsize=32)
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.savefig(filename)

#
